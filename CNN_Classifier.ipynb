{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "",
  "signature": "sha256:b5be1cc0705c70f1ab8e72058e7bec1618d5987ed4fb959190501afaf180bee3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import matplotlib.pyplot as plt\n",
      "from tensorflow.python.framework import ops\n",
      "from tensorflow.python.ops import clip_ops\n",
      "from bn_class import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\"\"\"Hyperparameters\"\"\"\n",
      "num_filt_1 = 15     #Number of filters in first conv layer\n",
      "num_filt_2 = 8      #Number of filters in second conv layer\n",
      "num_filt_3 = 8      #Number of filters in thirs conv layer\n",
      "num_fc_1 = 40       #Number of neurons in hully connected layer\n",
      "max_iterations = 5000\n",
      "batch_size = 100\n",
      "dropout = 0.5       #Dropout rate in the fully connected layer\n",
      "plot_row = 5        #How many rows do you want to plot in the visualization\n",
      "regularization = 1e-4\n",
      "learning_rate = 2e-3\n",
      "input_norm = False   # Do you want z-score input normalization?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = \"2330\"\n",
      "datadir = 'data/'+ dataset\n",
      "data_train = np.loadtxt(datadir+'_train_rsi',delimiter=',')\n",
      "data_test_val = np.loadtxt(datadir+'_test_rsi',delimiter=',')\n",
      "data_test,data_val = np.split(data_test_val,2)#upper for acc_test, lower for val_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "collapsed": true
     },
     "source": [
      "split training and testing data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = data_train[:,1:]\n",
      "X_val = data_val[:,1:]\n",
      "X_test = data_test[:,1:]\n",
      "N = X_train.shape[0]\n",
      "Ntest = X_test.shape[0]\n",
      "D = X_train.shape[1]\n",
      "y_train = data_train[:,0]\n",
      "y_val = data_val[:,0]\n",
      "y_test = data_test[:,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "normalize x and y"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "num_classes = len(np.unique(y_train))\n",
      "base = np.min(y_train)  #Check if data is 0-based\n",
      "if base != 0:\n",
      "    y_train -=base\n",
      "    y_val -= base\n",
      "    y_test -= base\n",
      "\n",
      "if input_norm:\n",
      "    mean = np.mean(X_train,axis=0)\n",
      "    variance = np.var(X_train,axis=0)\n",
      "    X_train -= mean\n",
      "    #The 1e-9 avoids dividing by zero\n",
      "    X_train /= np.sqrt(variance)+1e-9\n",
      "    X_val -= mean\n",
      "    X_val /= np.sqrt(variance)+1e-9\n",
      "    X_test -= mean\n",
      "    X_test /= np.sqrt(variance)+1e-9"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = np.floor(batch_size*max_iterations / N)\n",
      "print('Train with approximately %d epochs' %(epochs))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train with approximately 508 epochs\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "place for the input variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "x = tf.placeholder(\"float\", shape=[None, D], name = 'Input_data')\n",
      "y_ = tf.placeholder(tf.int64, shape=[None], name = 'Ground_truth')\n",
      "keep_prob = tf.placeholder(\"float\")\n",
      "bn_train = tf.placeholder(tf.bool)          #Boolean value to guide batchnorm"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "w and b and conv function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def weight_variable(shape, name):\n",
      "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
      "  return tf.Variable(initial, name = name)\n",
      "\n",
      "def bias_variable(shape, name):\n",
      "  initial = tf.constant(0.1, shape=shape)\n",
      "  return tf.Variable(initial, name = name)\n",
      "\n",
      "def conv2d(x, W):\n",
      "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
      "\n",
      "def max_pool_2x2(x):\n",
      "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
      "                        strides=[1, 2, 2, 1], padding='SAME')\n",
      "\n",
      "with tf.name_scope(\"Reshaping_data\") as scope:\n",
      "  x_image = tf.reshape(x, [-1,D,1,1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Build the graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ewma is the decay for which we update the moving average of the \n",
      "# mean and variance in the batch-norm layers\n",
      "with tf.name_scope(\"Conv1\") as scope:\n",
      "  W_conv1 = weight_variable([4, 1, 1, num_filt_1], 'Conv_Layer_1')\n",
      "  b_conv1 = bias_variable([num_filt_1], 'bias_for_Conv_Layer_1')\n",
      "  a_conv1 = conv2d(x_image, W_conv1) + b_conv1\n",
      "  \n",
      "with tf.name_scope('Batch_norm_conv1') as scope:\n",
      "    ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
      "    bn_conv1 = ConvolutionalBatchNormalizer(num_filt_1, 0.001, ewma, True)           \n",
      "    update_assignments = bn_conv1.get_assigner() \n",
      "    a_conv1 = bn_conv1.normalize(a_conv1, train=bn_train) \n",
      "    h_conv1 = tf.nn.relu(a_conv1)\n",
      "  \n",
      "with tf.name_scope(\"Conv2\") as scope:\n",
      "  W_conv2 = weight_variable([4, 1, num_filt_1, num_filt_2], 'Conv_Layer_2')\n",
      "  b_conv2 = bias_variable([num_filt_2], 'bias_for_Conv_Layer_2')\n",
      "  a_conv2 = conv2d(h_conv1, W_conv2) + b_conv2\n",
      "  \n",
      "with tf.name_scope('Batch_norm_conv2') as scope:\n",
      "    bn_conv2 = ConvolutionalBatchNormalizer(num_filt_2, 0.001, ewma, True)           \n",
      "    update_assignments = bn_conv2.get_assigner() \n",
      "    a_conv2 = bn_conv2.normalize(a_conv2, train=bn_train) \n",
      "    h_conv2 = tf.nn.relu(a_conv2)\n",
      "    \n",
      "with tf.name_scope(\"Conv3\") as scope:\n",
      "  W_conv3 = weight_variable([4, 1, num_filt_2, num_filt_3], 'Conv_Layer_3')\n",
      "  b_conv3 = bias_variable([num_filt_3], 'bias_for_Conv_Layer_3')\n",
      "  a_conv3 = conv2d(h_conv2, W_conv3) + b_conv3\n",
      "  \n",
      "with tf.name_scope('Batch_norm_conv3') as scope:\n",
      "    bn_conv3 = ConvolutionalBatchNormalizer(num_filt_3, 0.001, ewma, True)           \n",
      "    update_assignments = bn_conv3.get_assigner() \n",
      "    a_conv3 = bn_conv3.normalize(a_conv3, train=bn_train) \n",
      "    h_conv3 = tf.nn.relu(a_conv3)\n",
      "\n",
      "with tf.name_scope(\"Fully_Connected1\") as scope:\n",
      "  W_fc1 = weight_variable([D*num_filt_3, num_fc_1], 'Fully_Connected_layer_1')\n",
      "  b_fc1 = bias_variable([num_fc_1], 'bias_for_Fully_Connected_Layer_1')\n",
      "  h_conv3_flat = tf.reshape(h_conv3, [-1, D*num_filt_3])\n",
      "  h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
      "  \n",
      "with tf.name_scope(\"Fully_Connected2\") as scope:\n",
      "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
      "  W_fc2 = tf.Variable(tf.truncated_normal([num_fc_1, num_classes], stddev=0.1),name = 'W_fc2')\n",
      "  b_fc2 = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = 'b_fc2')\n",
      "  h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2   \n",
      "with tf.name_scope(\"SoftMax\") as scope:\n",
      "    regularizers = (tf.nn.l2_loss(W_conv1) + tf.nn.l2_loss(b_conv1) +\n",
      "                  tf.nn.l2_loss(W_conv2) + tf.nn.l2_loss(b_conv2) + \n",
      "                  tf.nn.l2_loss(W_conv3) + tf.nn.l2_loss(b_conv3) +\n",
      "                  tf.nn.l2_loss(W_fc1) + tf.nn.l2_loss(b_fc1) + \n",
      "                  tf.nn.l2_loss(W_fc2) + tf.nn.l2_loss(b_fc2))\n",
      "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(h_fc2,y_)\n",
      "    cost = tf.reduce_sum(loss) / batch_size\n",
      "    cost += regularization*regularizers\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-70655a6c4d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbn_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolutionalBatchNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_filt_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mupdate_assignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_conv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_assigner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0ma_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_conv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_conv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mh_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_conv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/airchen/Documents/coding/stock/bn_class.pyc\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m\"\"\"Returns a batch-normalized version of x.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0massign_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m       \u001b[0;34m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \"\"\"\n\u001b[0;32m--> 528\u001b[0;31m     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\u001b[0m\u001b[1;32m    529\u001b[0m                     \u001b[0;34m\"Use `if t is not None:` instead of `if t:` to test if a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                     \u001b[0;34m\"tensor is defined, and use the logical TensorFlow ops \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor."
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "define train optimizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tf.name_scope(\"train\") as scope:\n",
      "    tvars = tf.trainable_variables()\n",
      "    #We clip the gradients to prevent explosion\n",
      "    grads = tf.gradients(cost, tvars)\n",
      "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
      "    gradients = zip(grads, tvars)\n",
      "    train_step = optimizer.apply_gradients(gradients)\n",
      "\n",
      "    numel = tf.constant([[0]])\n",
      "    for gradient, variable in gradients:\n",
      "      if isinstance(gradient, ops.IndexedSlices):\n",
      "        grad_values = gradient.values\n",
      "      else:\n",
      "        grad_values = gradient\n",
      "      \n",
      "      numel +=tf.reduce_sum(tf.size(variable))  \n",
      "with tf.name_scope(\"Evaluating_accuracy\") as scope:\n",
      "    correct_prediction = tf.equal(tf.argmax(h_fc2,1), y_)\n",
      "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'cost' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-56358908f701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#We clip the gradients to prevent explosion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'cost' is not defined"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "run session and evaluate performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "perf_collect = np.zeros((3,int(np.floor(max_iterations /100))))\n",
      "with tf.Session() as sess:\n",
      "  sess.run(tf.initialize_all_variables())\n",
      "  \n",
      "  step = 0      # Step is a counter for filling the numpy array perf_collect\n",
      "  for i in range(max_iterations):#training process\n",
      "    batch_ind = np.random.choice(N,batch_size,replace=False)\n",
      "    \n",
      "    if i==0:\n",
      "        acc_test_before = sess.run(accuracy, feed_dict={ x: X_test, y_: y_test, keep_prob: 1.0, bn_train : False})\n",
      "    if i%100 == 0:\n",
      "      #Check training performance\n",
      "      result = sess.run(accuracy,feed_dict = { x: X_train, y_: y_train, keep_prob: 1.0, bn_train : False})\n",
      "      perf_collect[1,step] = result \n",
      "        \n",
      "      #Check validation performance\n",
      "      result = sess.run(accuracy, feed_dict={ x: X_val, y_: y_val, keep_prob: 1.0, bn_train : False})\n",
      "      acc = result\n",
      "      perf_collect[0,step] = acc    \n",
      "      print(\" Validation accuracy at %s out of %s is %s\" % (i,max_iterations, acc))\n",
      "      step +=1\n",
      "    sess.run(train_step,feed_dict={x:X_train[batch_ind], y_: y_train[batch_ind], keep_prob: dropout, bn_train : True})\n",
      "    \n",
      "          #training process done!\n",
      "  result = sess.run([accuracy,numel], feed_dict={ x: X_test, y_: y_test, keep_prob: 1.0, bn_train : False})\n",
      "  predict=sess.run(tf.argmax(h_fc2,1), feed_dict={ x: X_test, y_: y_test, keep_prob: 1.0, bn_train : False})\n",
      "  print(\"pred \"+\"real\")  \n",
      "  for x in xrange(0,len(predict)):\n",
      "    print(str(predict[x]+1)+\"    \"+str(int(y_test[x]+1)))\n",
      "\n",
      "  #for x in xrange(0,len(predict)):\n",
      "  #  if(predict[x]==0):\n",
      "  #    if(predict[x]==int(y_test[x])):\n",
      "  #      print(\"1\")#correct\n",
      "  #    else:\n",
      "  #      print(\"-1\")#wrong\n",
      "  #  else:\n",
      "  #      print(\"0\") \n",
      "\n",
      "  #print(sess.run(correct_prediction, feed_dict={ x: X_test, y_: y_test, keep_prob: 1.0, bn_train : False}))\n",
      "  acc_test = result[0]\n",
      "  print('The network has %s trainable parameters'%(result[1]))"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'accuracy' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-c6ebebe4725f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0macc_test_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_train\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m#Check training performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "show the graph of validation accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#2 for drop or same, 1 for rise \n",
      "print('The accuracy on the test data is %.3f, before training was %.3f' %(acc_test,acc_test_before))\n",
      "plt.figure()\n",
      "plt.plot(perf_collect[0],label='Valid accuracy')\n",
      "plt.plot(perf_collect[1],label = 'Train accuracy')\n",
      "plt.axis([0, step, 0, np.max(perf_collect)])\n",
      "#plt.show()\n",
      "plt.figure()"
     ],
     "language": "python",
     "metadata": {
      "scrolled": false
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'acc_test' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-13-4ac151f1fed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#2 for drop or same, 1 for rise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The accuracy on the test data is %.3f, before training was %.3f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_test_before\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperf_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperf_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Train accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'acc_test' is not defined"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}