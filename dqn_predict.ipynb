{
 "metadata": {
  "name": "",
  "signature": "sha256:81835088c7414662c7251f519a43db439d8e948714c72058b478cafa4ca485b8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import Dependencies\n",
      "\n",
      "from __future__ import print_function\n",
      "import gym\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import random\n",
      "from collections import deque\n",
      "from yahoo_finance import Share\n",
      "import datetime\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Constants\n",
      "\n",
      "EPISODE = 2000 # Total episodes to run\n",
      "STEP = 300 # Step limitation in an episode\n",
      "TEST = 10 # Test episode\n",
      "\n",
      "GAMMA = 0.9 # discount factor\n",
      "BUFFER_SIZE = 10000 # the replay buffer size\n",
      "BATCH_SIZE = 32 # minibatch size\n",
      "INITIAL_EPSILON = 0.5 # initial large, in order to explore more\n",
      "FINAL_EPSILON = 0.01 # explore less, greedy most time\n",
      "DAY_LENGTH = 10 # the total days for a training data, also the dim of features\n",
      "START = \"2011-01-01\"\n",
      "_ID = 2330 # By default, TSMC\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download stock data\n",
      "\n",
      "stock = Share(str(_ID)+'.TW')\n",
      "today = datetime.date.today()\n",
      "print(\"Getting data from yahoo_finance...\")\n",
      "stock_data = stock.get_historical(START, str(today))\n",
      "print(\"Finished fetching data\")\n",
      "print(\"Historical data since\", START,\": \", len(stock_data))\n",
      "stock_data.reverse()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Getting data from yahoo_finance...\n",
        "Finished fetching data"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Historical data since 2011-01-01 :  1447\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remove 0 volume data\n",
      "\n",
      "i = 0\n",
      "while( i < len(stock_data)):\n",
      "    if (int(stock_data[i].get('Volume')) <= 0):\n",
      "        stock_data.remove(stock_data[i])\n",
      "        i = -1\n",
      "    i += 1\n",
      "\n",
      "print(\"Remove the datas with zero volume, total data \",len(stock_data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Remove the datas with zero volume, total data  1369\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get close price\n",
      "\n",
      "close = np.zeros((len(stock_data)-DAY_LENGTH, DAY_LENGTH), dtype=np.float)\n",
      "for i in range(0, len(close)):\n",
      "    for j in range(0, DAY_LENGTH):\n",
      "        close[i,j] = float(stock_data[i+j].get('Close'))\n",
      "print (close)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  71.1   71.2   69.8 ...,   74.9   74.6   74.8]\n",
        " [  71.2   69.8   71.  ...,   74.6   74.8   75.8]\n",
        " [  69.8   71.    72.5 ...,   74.8   75.8   76.8]\n",
        " ..., \n",
        " [ 173.   175.   176.5 ...,  174.5  177.   178. ]\n",
        " [ 175.   176.5  172.5 ...,  177.   178.   178.5]\n",
        " [ 176.5  172.5  177.  ...,  178.   178.5  179.5]]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TWStock():\n",
      "\tdef __init__(self, stock_data):\n",
      "\t\tself.stock_data = stock_data\n",
      "\t\tself.train_data = self.stock_data[0:(len(stock_data)*3)//5]\n",
      "\t\tself.test_data = self.stock_data[(len(stock_data)*3)//5:len(stock_data)]\n",
      "\t\tself.stock_index = 0\n",
      "\t\tprint(\"Training Data: \",len(self.train_data))\n",
      "\t\tprint(\"Testing Data: \",len(self.test_data))\n",
      "\n",
      "\tdef render(self):\n",
      "\t\treturn\n",
      "\n",
      "\tdef train_reset(self):\n",
      "\t\tself.stock_index = 0\n",
      "\t\treturn self.train_data[self.stock_index]\n",
      "\n",
      "\tdef test_reset(self):\n",
      "\t\tself.stock_index = 0\n",
      "\t\treturn self.test_data[self.stock_index]\n",
      "\t\t\n",
      "\t# 0: observe, 1: having stock, 2: no stock\n",
      "\tdef train_step(self,action): # for training, feed training data\n",
      "\t\tself.stock_index+=1\n",
      "\t\taction_reward = self.train_data[self.stock_index][DAY_LENGTH-1] - self.train_data[self.stock_index][DAY_LENGTH-2]\n",
      "\t\tif action == 0:\n",
      "\t\t\taction_reward = 0\n",
      "\t\telif action == 2:\n",
      "\t\t\taction_reward = -1*action_reward\n",
      "\t\tstock_done = False\n",
      "\t\tif(self.stock_index)>= len(self.train_data)-1:\n",
      "\t\t\tstock_done = True\n",
      "        \telse:\n",
      "           \t\tstock_done = False\n",
      "\t\treturn self.train_data[self.stock_index], action_reward, stock_done, 0\n",
      "\n",
      "\tdef test_step(self,action): # for testing, feed testing data\n",
      "\t\tself.stock_index+=1\n",
      "\t\taction_reward = self.test_data[self.stock_index][DAY_LENGTH-1] - self.test_data[self.stock_index][DAY_LENGTH-2]\n",
      "\t\tif action == 0:\n",
      "\t\t\taction_reward = 0\n",
      "\t\telif action == 2:\n",
      "\t\t\taction_reward = -1*action_reward\n",
      "\t\tstock_done = False\n",
      "\t\tif(self.stock_index)>= len(self.test_data)-1:\n",
      "\t\t\tstock_done = True\n",
      "        \telse:\n",
      "           \t\tstock_done = False\n",
      "\t\treturn self.test_data[self.stock_index], action_reward, stock_done, 0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DQN():\n",
      "\tdef __init__(self,env):\n",
      "\t\t# experience replay\n",
      "\t\tself.replay_buffer = deque()\n",
      "\t\t# initialize parameters\n",
      "\t\tself.time_step = 0\n",
      "\t\tself.epsilon = INITIAL_EPSILON\n",
      "\t\tself.state_dim = DAY_LENGTH\n",
      "\t\tself.action_dim = 3\n",
      "\t\t\n",
      "\t\tself.create_Q_network()\n",
      "\t\tself.create_training_method()\n",
      "\n",
      "\t\t# create session\n",
      "\t\t#g_record = tf.Graph()\n",
      "\t\t#self.g_session = tf.InteractiveSession(graph=g_record)\n",
      "\t\tself.t_session = tf.InteractiveSession()\n",
      "\n",
      "\t\t#with g_record.as_default():\n",
      "\t\tself.R = tf.placeholder(\"float\", shape = None)\n",
      "\t\tR_summ = tf.scalar_summary(tags =\"reward\", values = self.R)\n",
      "\n",
      "\t\tself.merged_summ = tf.merge_all_summaries()\n",
      "\t\tself.writer = tf.train.SummaryWriter(logdir = \"/home/airchen/Documents/coding/stock\", graph = self.t_session.graph)\n",
      "\n",
      "\t\t\n",
      "\t\tself.t_session.run(tf.initialize_all_variables())\n",
      "\t\n",
      "\tdef get_summ(self):\n",
      "\t\treturn self.t_session, self.merged_summ, self.R, self.writer\n",
      "\n",
      "\tdef create_Q_network(self): \n",
      "\t\t'''\n",
      "\t\t# Use MLP\n",
      "\t\t# weights and biase\n",
      "\t\tW1 = tf.Variable(tf.truncated_normal([self.state_dim,20]))\n",
      "\t\tb1 = tf.Variable(tf.constant(0.01, shape = [20]))\n",
      "\t\tW2 = tf.Variable(tf.truncated_normal([20,self.action_dim]))\n",
      "\t\tb2 = tf.Variable(tf.constant(0.01, shape = [self.action_dim]))\n",
      "\n",
      "\t\t# Layer implementation\n",
      "\t\tself.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n",
      "\t\thidden = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n",
      "\t\tself.Q_value = tf.matmul(hidden,W2) + b2\n",
      "\t\t'''\n",
      "\t\t# Use CNN\n",
      "\t\t# weights and biases\n",
      "\t\tW_conv1 = tf.Variable(tf.truncated_normal(shape = [4,1,1,10],stddev = 0.01))\n",
      "\t\tb_conv1 = tf.Variable(tf.constant(0.01,shape = [10]))\n",
      "\t\tW_conv2 = tf.Variable(tf.truncated_normal(shape = [4,1,10,40],stddev = 0.01))\n",
      "\t\tb_conv2 = tf.Variable(tf.constant(0.01,shape = [40]))\n",
      "\t\tW_fc = tf.Variable(tf.truncated_normal(shape = [400,self.action_dim],stddev = 0.01))\n",
      "\t\tb_fc = tf.Variable(tf.constant(0.01,shape = [self.action_dim]))\n",
      "\n",
      "\t\t# Layer implementation\n",
      "\t\tself.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n",
      "\t\tx = tf.reshape(self.state_input,[-1,10,1,1])\n",
      "\t\th_conv1 = tf.nn.relu(tf.nn.conv2d(x,W_conv1,strides = [1,1,1,1],padding = 'SAME') + b_conv1)\n",
      "\t\th_conv2 = tf.nn.relu(tf.nn.conv2d(h_conv1,W_conv2,strides = [1,1,1,1],padding = 'SAME') + b_conv2)\n",
      "\t\thidden = tf.reshape(h_conv2,[-1,400])\n",
      "\t\tself.Q_value = tf.matmul(hidden,W_fc) + b_fc\n",
      "\t\t\n",
      "\n",
      "\tdef create_training_method(self):\n",
      "\t\tself.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot key vector\n",
      "\t\tself.y_input = tf.placeholder(\"float\",[None])\n",
      "\t\tQ_action = tf.reduce_sum(tf.mul(self.Q_value,self.action_input), reduction_indices = 1)\n",
      "\t\tself.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
      "\t\tself.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
      "\n",
      "\tdef train_Q_network(self):\n",
      "\t\tself.time_step+=1\n",
      "\t\t# Step 1: obtain random minibatch from replay memory\n",
      "\t\tminibatch = random.sample(self.replay_buffer, BATCH_SIZE)\n",
      "\t\tstate_batch = [data[0] for data in minibatch]\n",
      "\t\taction_batch = [data[1] for data in minibatch]\n",
      "\t\treward_batch = [data[2] for data in minibatch]\n",
      "\t\tnext_state_batch = [data[3] for data in minibatch]\n",
      "\n",
      "\t\t# step 2: calculate y\n",
      "\t\ty_batch = []\n",
      "\t\tQ_value_batch = self.Q_value.eval(feed_dict = {self.state_input:next_state_batch})\n",
      "\t\tfor i in range(0,BATCH_SIZE):\n",
      "\t\t\tdone = minibatch[i][4]\n",
      "\t\t\tif done:\n",
      "\t\t\t\ty_batch.append(reward_batch[i])\n",
      "\t\t\telse:\n",
      "\t\t\t\ty_batch.append(reward_batch[i] + GAMMA*np.max(Q_value_batch[i]))\n",
      "\t\tself.optimizer.run(feed_dict = {\n",
      "\t\t\tself.y_input:y_batch, \n",
      "\t\t\tself.action_input:action_batch, \n",
      "\t\t\tself.state_input:state_batch})\n",
      "\n",
      "\tdef egreedy_action(self,state): # during training \n",
      "\t\tQ_value = self.Q_value.eval(feed_dict = {self.state_input:[state]},session = self.t_session)[0] # Unknown\n",
      "\t\tif random.random() <= self.epsilon:\n",
      "\t\t\treturn random.randint(0,self.action_dim-1)\n",
      "\t\telse:\n",
      "\t\t\treturn np.argmax(Q_value)\n",
      "\t\tself.epsilon -= (Initial_EPSILON-FINAL_EPSILON)/10000\n",
      "\t\n",
      "\tdef action(self,state): # during testing\n",
      "\t\treturn np.argmax(self.Q_value.eval(feed_dict = {self.state_input:[state]})[0])\n",
      "\t\t\n",
      "\tdef perceive(self,state,action,reward,next_state,done):\n",
      "\t\t# assign the to be made action into a one hot vector\n",
      "\t\tone_hot_action = np.zeros(self.action_dim)\n",
      "\t\tone_hot_action[action] = 1\n",
      "\t\tself.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
      "\t\tif len(self.replay_buffer) > BUFFER_SIZE:\n",
      "\t\t\tself.replay_buffer.popleft()\n",
      "\t\tif len(self.replay_buffer) > BATCH_SIZE:\n",
      "\t\t\tself.train_Q_network()\n",
      "def main():\n",
      "        env = TWStock(close) # Initialize environment\n",
      "        agent = DQN(env) # Initialize dqn agent\n",
      "        sess,merged,R,writer = agent.get_summ()\n",
      "\n",
      "        for episode in xrange(EPISODE):\n",
      "                state = env.train_reset() # reset() returns observation\n",
      "                # Training\n",
      "                for step in xrange(STEP):\n",
      "                        action = agent.egreedy_action(state) # e-greedy action for training\n",
      "                        next_state,reward,done,info = env.train_step(action)\n",
      "                        agent.perceive(state,action,reward,next_state,done)\n",
      "                        state = next_state\n",
      "                        if done:\n",
      "                                break\n",
      "                # Testing\n",
      "                if episode % 20 == 0:\n",
      "                        total_reward = 0\n",
      "                        for i in xrange(TEST):\n",
      "                                state = env.test_reset()\n",
      "                                for j in xrange(STEP):\n",
      "                                        env.render()\n",
      "                                        action = agent.action(state) # direct action for test\n",
      "                                        state, reward, done, info = env.test_step(action)\n",
      "                                        total_reward += reward\n",
      "                                        if done:\n",
      "                                                break\n",
      "                        avg_reward = total_reward/TEST\n",
      "                        print (\"Episode: \", episode, \" Evaluation Average Reward: \",avg_reward)\n",
      "                        record = sess.run(merged, feed_dict={R:avg_reward})\n",
      "                        writer.add_summary(record, global_step = episode)\n",
      "                        writer.flush()\n",
      "                        if avg_reward >= 200:\n",
      "                                break\n",
      "\n",
      "if __name__ == '__main__':\n",
      "        main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data:  815\n",
        "Testing Data:  544\n",
        "Episode: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0  Evaluation Average Reward:  10.0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7fdaad1a6990>> ignored\n"
       ]
      },
      {
       "ename": "InvalidArgumentError",
       "evalue": "You must feed a value for placeholder tensor 'Placeholder_3' with dtype float\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder_3', defined at:\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n    app.start()\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-dc022137da92>\", line 37, in <module>\n    main()\n  File \"<ipython-input-9-dc022137da92>\", line 3, in main\n    agent = DQN(env) # Initialize dqn agent\n  File \"<ipython-input-8-7043a51bd993>\", line 17, in __init__\n    self.R = tf.placeholder(\"float\", shape = None)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 895, in placeholder\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1238, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-e838ce376ce3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-20-e838ce376ce3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m                         \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Evaluation Average Reward: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mavg_reward\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_3' with dtype float\n\t [[Node: Placeholder_3 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder_3', defined at:\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 469, in main\n    app.start()\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/kernelapp.py\", line 459, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 245, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python2.7/dist-packages/IPython/kernel/zmq/ipkernel.py\", line 389, in execute_request\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\n    interactivity=interactivity, compiler=compiler)\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code):\n  File \"/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-dc022137da92>\", line 37, in <module>\n    main()\n  File \"<ipython-input-9-dc022137da92>\", line 3, in main\n    agent = DQN(env) # Initialize dqn agent\n  File \"<ipython-input-8-7043a51bd993>\", line 17, in __init__\n    self.R = tf.placeholder(\"float\", shape = None)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 895, in placeholder\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1238, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}