{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.finance as finance\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from yahoo_finance import Share\n",
    "import datetime\n",
    "from bn_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_len = 10    # numbers of days for every data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of data: 261\n",
      "after removing the datas with zero volume, the length of data: 242\n",
      "the length of data: 160\n",
      "after removing the datas with zero volume, the length of data: 146\n"
     ]
    }
   ],
   "source": [
    "def get_stock(ticker, startdate, enddate):\n",
    "    fh = finance.fetch_historical_yahoo(ticker, startdate, enddate)\n",
    "    # a numpy record array with fields: date, open, high, low, close, volume, adj_close)\n",
    "    r = mlab.csv2rec(fh)\n",
    "    fh.close()\n",
    "    r.sort()\n",
    "    print 'the length of data:', len(r.close)\n",
    "    get_stock_data = []\n",
    "    for i in xrange(0, len(r.close)-1):\n",
    "        if (r.volume[i] != 0):\n",
    "            get_stock_data.append(r.close[i].tolist())\n",
    "    print 'after removing the datas with zero volume, the length of data:', len(get_stock_data)\n",
    "    return get_stock_data\n",
    "\n",
    "ticker = '2330.TW'\n",
    "\n",
    "train = get_stock(ticker, datetime.date(2015, 1, 1), datetime.date(2015, 12, 31))\n",
    "test = get_stock(ticker, datetime.date(2016, 1, 1), datetime.date.today())\n",
    "max_ylim = max(max(train), max(test))\n",
    "min_ylim = min(min(train), min(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def save_pic(data, filename):\n",
    "    for i in xrange (0, len(data)-day_len):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "        fig.set_size_inches(1, 1)\n",
    "        ax.plot([i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9], [data[i], data[i+1], data[i+2], data[i+3], data[i+4], data[i+5], data[i+6], data[i+7], data[i+8], data[i+9]])\n",
    "        ax.set_ylim([min_ylim, max_ylim])\n",
    "        plt.axis('off')\n",
    "        fig.savefig('/home/carine/Desktop/tmp/'+filename+'/'+filename+'_'+str(i)+'.png', dpi=80)\n",
    "        fig.clear()\n",
    "        plt.close(fig)\n",
    "\n",
    "save_pic(train, \"train\")\n",
    "save_pic(test, \"test\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_image(file_dir):\n",
    "    img = mpimg.imread(file_dir)\n",
    "    return img\n",
    "\n",
    "image = []\n",
    "for i in xrange(0, len(train)-day_len):\n",
    "    file_dir = \"/home/carine/Desktop/tmp/train/train_\" + str(i) + \".png\"\n",
    "    image.append(get_image(file_dir))\n",
    "my_train = np.asarray(image)\n",
    "\n",
    "image = []\n",
    "for i in xrange(0, len(test)-day_len):\n",
    "    file_dir = \"/home/carine/Desktop/tmp/test/test_\" + str(i) + \".png\"\n",
    "    image.append(get_image(file_dir))\n",
    "my_test = np.asarray(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_stock_train = np.zeros((len(train)-day_len, day_len), dtype=np.float)\n",
    "my_stock_test = np.zeros((len(test)-day_len, day_len), dtype=np.float)\n",
    "for i in xrange(0, len(my_stock_train)):\n",
    "    for j in xrange(0, day_len):\n",
    "        my_stock_train[i,j] = train[i+j]\n",
    "\n",
    "for i in xrange(0, len(my_stock_test)):\n",
    "    for j in xrange(0, day_len):\n",
    "        my_stock_test[i,j] = test[i+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TWStock():\n",
    "    def __init__(self, image_data, stock_price):\n",
    "        self.image_data = image_data\n",
    "        self.stock_price = stock_price\n",
    "        self.stock_index = 0\n",
    "    \n",
    "    def render(self):\n",
    "        return \n",
    "    \n",
    "    def reset(self):\n",
    "        self.stock_index = 0\n",
    "        return self.image_data[self.stock_index]\n",
    "    \n",
    "\n",
    "    def step(self, action): \n",
    "        self.stock_index += 1\n",
    "        action_reward = self.stock_price[self.stock_index][day_len-1] - self.stock_price[self.stock_index][day_len-2] \n",
    "        #action_reward = self.stock_price[self.stock_index+9] - self.stock_price[self.stock_index+8]\n",
    "        if (action == 0):\n",
    "            action_reward = 0\n",
    "        if (action == 2):\n",
    "            action_reward = -1 * action_reward\n",
    "\n",
    "        stock_done = False\n",
    "        if self.stock_index >= len(self.image_data)-1:\n",
    "            stock_done = True\n",
    "        else:\n",
    "            stock_done = False\n",
    "        return self.image_data[self.stock_index], action_reward, stock_done, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    # DQN Agent\n",
    "    def __init__(self, env):\n",
    "        # init experience replay\n",
    "        self.replay_buffer = deque()\n",
    "\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "    \n",
    "        #self.state_dim = env.observation_space.shape[0]\n",
    "        #self.action_dim = env.action_space.n\n",
    "    \n",
    "        self.state_dim = [1, 80, 80, 4]\n",
    "        self.action_dim = 3\n",
    "\n",
    "\n",
    "        self.create_Q_network()\n",
    "        self.create_training_method()\n",
    "\n",
    "\n",
    "        #g_record = tf.Graph()\n",
    "        #self.g_session = tf.InteractiveSession(graph=g_record)\n",
    "        self.t_session = tf.InteractiveSession()\n",
    "\n",
    "        #with g_record.as_default():\n",
    "        #self.R = tf.placeholder(\"float\", shape = None)\n",
    "        #R_summ = tf.scalar_summary(tags =\"reward\", values = self.R)\n",
    "\n",
    "        #self.merged_summ = tf.merge_all_summaries()\n",
    "        \n",
    "        #self.writer = tf.train.SummaryWriter('/home/carine/Desktop/stock_DQN/eventlog',graph=self.t_session.graph)\n",
    "\n",
    "        self.t_session.run(tf.initialize_all_variables())\n",
    "\n",
    "    #def get_summ(self):\n",
    "        #return self.t_session, self.merged_summ, self.R, self.writer\n",
    "\n",
    "    def create_Q_network(self):\n",
    "        # input layer\n",
    "        self.state_input = tf.placeholder(tf.float32,[None ,80, 80, 4])\n",
    "\n",
    "        \n",
    "        # network weights\n",
    "        W_conv1 = self.weight_variable([8, 8, 4, 32])\n",
    "        b_conv1 = self.bias_variable([32])\n",
    "        \n",
    "        W_conv2 = self.weight_variable([4, 4, 32, 64])\n",
    "        b_conv2 = self.bias_variable([64])\n",
    "\n",
    "        W_conv3 = self.weight_variable([3, 3, 64, 64])\n",
    "        b_conv3 = self.bias_variable([64])\n",
    "        \n",
    "        \n",
    "        W_fc1 = self.weight_variable([1600, 512])\n",
    "        b_fc1 = self.bias_variable([512])\n",
    "        \n",
    "        W_fc2 = self.weight_variable([512, self.action_dim])\n",
    "        b_fc2 = self.bias_variable([self.action_dim])\n",
    "        \n",
    "        # hidden layers\n",
    "        h_conv1 = tf.nn.relu(self.conv2d(self.state_input, W_conv1, 4) + b_conv1)\n",
    "        h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "        h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "        h_conv3 = tf.nn.relu(self.conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "        h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "        # Q Value layer\n",
    "        self.Q_value = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "        \n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(tf.float32,[None,self.action_dim])\n",
    "        # one hot presentation\n",
    "        self.y_input = tf.placeholder(tf.float32,[None])\n",
    "        Q_action = tf.reduce_sum(tf.mul(self.Q_value,self.action_input),reduction_indices = 1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n",
    "\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else :\n",
    "                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n",
    "\n",
    "        self.optimizer.run(feed_dict={\n",
    "          self.y_input: y_batch,\n",
    "          self.action_input: action_batch,\n",
    "          self.state_input: state_batch\n",
    "          })\n",
    "        \n",
    "\n",
    "    def egreedy_action(self,state):\n",
    "        Q_value = self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]})[0]\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0,self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "\n",
    "        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/10000\n",
    "\n",
    "\n",
    "    def action(self,state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]})[0])\n",
    "\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(self, x, W, stride):\n",
    "        return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "ENV_NAME = 'stock_2330-v0' \n",
    "EPISODE = 2000 #10000 # Episode limitation\n",
    "STEP = 300 #1000   #300 # Step limitation in an episode\n",
    "TEST = 10 #10 # The number of experiment test every 100 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    #env = gym.make(ENV_NAME)\n",
    "    env = TWStock(my_train,my_stock_train) \n",
    "    agent = DQN(env)\n",
    "    #sess,merged,R,writer = agent.get_summ()\n",
    "\n",
    "    print 'Start!'\n",
    "    for episode in xrange(EPISODE):\n",
    "    \n",
    "        # initialize task\n",
    "        state = env.reset()\n",
    "\n",
    "        # Train\n",
    "        for step in xrange(STEP):\n",
    "            action = agent.egreedy_action(state) # e-greedy action for trai\n",
    "\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "\n",
    "            # Define reward for agent\n",
    "            reward_agent = -1 if done else 0.1\n",
    "            agent.perceive(state,action,reward,next_state,done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "\n",
    "        # Test every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            #env_test = TWStock(my_test, my_stock_test)\n",
    "            total_reward = 0\n",
    "\n",
    "            for i in xrange(TEST):\n",
    "                state = env.reset()\n",
    "\n",
    "                for j in xrange(STEP):\n",
    "                    env.render()\n",
    "                    action = agent.action(state)   # direct action for test\n",
    "                    state,reward,done,_ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "            ave_reward = total_reward/TEST\n",
    "            print 'episode: ',episode,'Evaluation Average Reward:',ave_reward\n",
    "            #record = sess.run(merged, feed_dict={R:ave_reward})\n",
    "            #writer.add_summary(record, global_step = episode)\n",
    "            #writer.flush()\n",
    "            #if ave_reward >= 200:\n",
    "            #    print 'Done!' \n",
    "            #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start!\n",
      "episode:  0 Evaluation Average Reward: -5.0\n",
      "episode:  100 Evaluation Average Reward: 5.0\n",
      "episode:  200 Evaluation Average Reward: -5.0\n",
      "episode:  300 Evaluation Average Reward: 5.0\n",
      "episode:  400 Evaluation Average Reward: 5.0\n",
      "episode:  500 Evaluation Average Reward: 5.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
